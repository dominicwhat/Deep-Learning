{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.read_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/test.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA's Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We're asking for you to think about concepts and different approaches for what we've described here,\" William Gerstenmaier, NASA's associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA's Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they'd win Congress' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public's money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA's other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA's plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon's orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX's Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    #r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    #emoticons = re.findall(r, text)\n",
    "    #text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    #text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' clara moskowitz for space com 2013 06 19 15 04 30 utc nasa s grand challenge stop asteroids from destroying earth there may be killer asteroids headed for earth and nasa has decided to do something about it the space agency announced a new grand challenge on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet the new mission builds on projects already underway at nasa including a plan to capture an asteroid pull it in toward the moon and send astronauts to visit it as part of the grand challenge the agency issued a request for information today aiming to solicit ideas from industry academia and the public on how to improve the asteroid mission plan we re asking for you to think about concepts and different approaches for what we ve described here william gerstenmaier nasa s associate administrator for human explorations and operations said yesterday during a nasa event announcing the initiative we want you to think about other ways of enhancing this to get the most out of it see also how it works nasa asteroid captureresponses to the request for information which also seeks ideas for detecting and mitigating asteroid threats are due july 18 the asteroid retrieval mission designed to provide the first deep space mission for astronauts flying on nasa s space launch system rocket and orion space capsule under development has come under fire from lawmakers who would prefer that nasa return to the moon a draft nasa authorization bill from the house space subcommittee which is currently in debate would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday june 19 at 10 a m edt see also how it works nasa asteroid capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident they d win congress support once they explained its benefits further i think that we really truly are going to be able to show the value of the mission nasa associate administrator lori garver said today to me this is something that what we do in this country we debate how we spend the public s money this is the beginning of the debate garver also maintained that sending astronauts to an asteroid would not diminish nasa s other science and exploration goals including another lunar landing see also animation of proposed asteroid retrieval mission this initiative takes nothing from the other valuable work she said this is only a small piece of our overall strategy but it is an integral piece it takes nothing from the moon part of nasa s plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threats if someday humanity discovers an asteroid headed for earth and manages to alter its course it will be one of the most important accomplishments in human history said tom kalil deputy director for technology and innovation at the white house office of science and technology policy see also wildest private deep space mission ideas a countdownthe topic of asteroid threats is more timely than ever after a meteor exploded over the russian city of chelyabinsk on feb 15 the same day that the football field sized asteroid 2012 da14 passed within the moon s orbit of earth image courtesy of nasa spacex s musk says sabotage unlikely cause of sept 1 explosion but still a worry proxima centauri is like our sun on steroids china launches shenzhou 11 astronauts to tiangong 2 space lab space station mockup in houston astronaut guided tour video this article originally published at space com here topics asteroid asteroids challenge earth space u s world  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0,'Page content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Stop-Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stat_pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW (Bag-Of-Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example documents]\n",
      "Study hard, then you will be happy and I will be happy\n",
      "\"I'm not happy :(\" \", because you don't study hard\n",
      "\n",
      "[vocabulary]\n",
      "{'clara': 33, 'moskowitz': 129, 'space': 181, 'com': 35, 'utc': 209, 'nasa': 131, 'grand': 88, 'challeng': 29, 'stop': 188, 'asteroid': 16, 'destroy': 55, 'earth': 65, 'may': 122, 'killer': 110, 'head': 90, 'decid': 49, 'someth': 180, 'agenc': 4, 'announc': 10, 'new': 132, 'june': 108, 'find': 77, 'danger': 46, 'rock': 163, 'figur': 76, 'planet': 147, 'mission': 124, 'build': 22, 'project': 151, 'alreadi': 6, 'underway': 207, 'includ': 100, 'plan': 146, 'captur': 25, 'pull': 158, 'toward': 204, 'moon': 128, 'send': 172, 'astronaut': 17, 'visit': 213, 'part': 142, 'issu': 106, 'request': 160, 'inform': 102, 'today': 200, 'aim': 5, 'solicit': 178, 'idea': 96, 'industri': 101, 'academia': 1, 'public': 156, 'improv': 99, 'ask': 14, 'think': 196, 'concept': 37, 'differ': 58, 'approach': 12, 'describ': 53, 'william': 219, 'gerstenmai': 84, 'associ': 15, 'administr': 3, 'human': 95, 'explor': 72, 'oper': 137, 'said': 167, 'yesterday': 226, 'event': 68, 'initi': 103, 'want': 214, 'way': 215, 'enhanc': 67, 'get': 85, 'see': 170, 'also': 7, 'work': 222, 'capturerespons': 26, 'seek': 171, 'detect': 56, 'mitig': 125, 'threat': 197, 'due': 64, 'juli': 107, 'retriev': 161, 'design': 54, 'provid': 154, 'first': 79, 'deep': 50, 'fli': 80, 'launch': 114, 'system': 193, 'rocket': 164, 'orion': 140, 'capsul': 24, 'develop': 57, 'come': 36, 'fire': 78, 'lawmak': 115, 'would': 225, 'prefer': 149, 'return': 162, 'draft': 63, 'author': 18, 'bill': 21, 'hous': 93, 'subcommitte': 190, 'current': 44, 'debat': 48, 'cancel': 23, 'steer': 185, 'discuss': 62, 'hear': 91, 'wednesday': 216, 'edt': 66, 'picturesbut': 144, 'offici': 135, 'defend': 51, 'confid': 38, 'win': 220, 'congress': 39, 'support': 192, 'explain': 70, 'benefit': 20, 'realli': 159, 'truli': 205, 'go': 86, 'abl': 0, 'show': 175, 'valu': 210, 'lori': 118, 'garver': 83, 'countri': 41, 'spend': 183, 'money': 127, 'begin': 19, 'maintain': 120, 'diminish': 59, 'scienc': 169, 'goal': 87, 'anoth': 11, 'lunar': 119, 'land': 112, 'anim': 9, 'propos': 152, 'take': 194, 'noth': 133, 'valuabl': 211, 'small': 177, 'piec': 145, 'overal': 141, 'strategi': 189, 'integr': 105, 'flight': 81, 'link': 117, 'close': 34, 'larger': 113, 'protect': 153, 'someday': 179, 'discov': 61, 'manag': 121, 'alter': 8, 'cours': 42, 'one': 136, 'import': 98, 'accomplish': 2, 'histori': 92, 'tom': 201, 'kalil': 109, 'deputi': 52, 'director': 60, 'technolog': 195, 'innov': 104, 'white': 217, 'offic': 134, 'polici': 148, 'wildest': 218, 'privat': 150, 'countdownth': 40, 'topic': 202, 'time': 199, 'ever': 69, 'meteor': 123, 'explod': 71, 'russian': 165, 'citi': 32, 'chelyabinsk': 30, 'feb': 74, 'day': 47, 'footbal': 82, 'field': 75, 'size': 176, 'da14': 45, 'pass': 143, 'within': 221, 'orbit': 138, 'imag': 97, 'courtesi': 43, 'spacex': 182, 'musk': 130, 'say': 168, 'sabotag': 166, 'unlik': 208, 'caus': 27, 'sept': 173, 'explos': 73, 'still': 187, 'worri': 224, 'proxima': 155, 'centauri': 28, 'like': 116, 'sun': 191, 'steroid': 186, 'china': 31, 'shenzhou': 174, 'tiangong': 198, 'lab': 111, 'station': 184, 'mockup': 126, 'houston': 94, 'guid': 89, 'tour': 203, 'video': 212, 'articl': 13, 'origin': 139, 'publish': 157, 'u': 206, 'world': 223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stat_pc\\anaconda3\\envs\\python3.7.9v1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc_dummy = [\"Study hard, then you will be happy and I will be happy\", \n",
    "           \"\\\"I'm not happy :(\\\" \\\", because you don't study hard\"]\n",
    "print('[example documents]\\n{}\\n'.format('\\n'.join(doc_dummy)))\n",
    "\n",
    "# ngram_range=(min,max), default: 1-gram => (1,1)\n",
    "count = CountVectorizer(ngram_range=(1, 1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "count.fit([df.loc[0,'Page content']])\n",
    "# dictionary is stored in vocabulary_\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n{}'.format(BoW))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example documents]\n",
      "Study hard, then you will be happy and I will be happy\n",
      "\"I'm not happy :(\" \", because you don't study hard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "print('[example documents]\\n{}\\n'.format('\\n'.join(doc_dummy)))\n",
    "\n",
    "# hash words to 1024 buckets\n",
    "hashvec = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression pipe3 Kaggle 0.53452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    6.8s remaining:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.487 (+/-0.056)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression+(1,2)gram: 0.487 (+/-0.047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.5s remaining:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression+preprocess: 0.515 (+/-0.048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   18.1s remaining:   18.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression+preprocess+hash: 0.509 (+/-0.067)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.4s finished\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['LogisticRegression', \n",
    "         'LogisticRegression+(1,2)gram',\n",
    "         'LogisticRegression+preprocess',\n",
    "         'LogisticRegression+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', LogisticRegression(solver = \"liblinear\",max_iter=10000))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', LogisticRegression(solver = \"liblinear\",max_iter=10000))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', LogisticRegression(solver = \"liblinear\",max_iter=10000))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', LogisticRegression(solver = \"liblinear\",max_iter=10000))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(preprocessor=<function preprocessor at 0x000001A8F7E1F8B8>,\n",
       "                                 tokenizer=<function tokenizer_stem_nostop at 0x000001A89CB631F8>)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(max_iter=10000, solver='liblinear'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3.fit(df['Page content'],df['Popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe3.predict(testdata['Page content'])\n",
    "pred[pred==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred.csv',index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost pipe1 Kaggle 0.54126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   18.4s remaining:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.537 (+/-0.044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  4.1min remaining:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.532 (+/-0.049)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   23.5s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   24.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.495 (+/-0.059)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.9s remaining:   17.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.502 (+/-0.038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.2s finished\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier())])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier())])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier())])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('clf', XGBClassifier())])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(df['Page content'],df['Popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe1.predict(testdata['Page content'])\n",
    "pred[pred==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v2.csv',index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    6.3s remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.506 (+/-0.066)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   25.2s remaining:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.504 (+/-0.065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   18.1s remaining:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.506 (+/-0.051)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.8s remaining:   17.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.510 (+/-0.065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', svm.SVC())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', svm.SVC())])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', svm.SVC())])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', svm.SVC())])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    2.9s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   12.7s remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB+(1,2)gram: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   14.3s remaining:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   14.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB+preprocess: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB+preprocess+hash: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   14.3s remaining:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   14.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['GaussianNB', \n",
    "         'GaussianNB+(1,2)gram',\n",
    "         'GaussianNB+preprocess',\n",
    "         'GaussianNB+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', GaussianNB())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', GaussianNB())])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', GaussianNB())])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', GaussianNB())])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  4.5min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.496 (+/-0.067)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  4.3min remaining:  4.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.467 (+/-0.063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram']\n",
    "\n",
    "pipe1 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5),preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier())])\n",
    "\n",
    "pipe2 = Pipeline([('vect', HashingVectorizer(n_features=2**20,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier())])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost pipe2 Kaggle 0.54907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   16.8s remaining:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.544 (+/-0.050)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  3.6min remaining:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.549 (+/-0.052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   22.3s remaining:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.510 (+/-0.047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   18.3s remaining:   18.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.516 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.5s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =50, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =50, random_state= 0,subsample=0.5))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =50, random_state= 0,subsample=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =50, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model花了40分鐘左右...\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =50, random_state= 0,subsample=0.5))])\n",
    "\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "pred = pipe2.predict(testdata['Page content'])\n",
    "pred[pred==-1] = 0\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v3.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost pipe2 Kaggle 0.57054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   30.3s remaining:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   30.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.560 (+/-0.046)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 16.7min remaining: 16.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.564 (+/-0.041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 16.8min finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14:30~17:54\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,n_jobs=-1,verbose=3))])\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "pred = pipe2.predict_proba(testdata['Page content'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v5.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   33.0s remaining:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   33.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.509 (+/-0.048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   20.8s remaining:   20.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.504 (+/-0.038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   21.6s finished\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   46.5s remaining:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.559 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 25.1min remaining: 25.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.565 (+/-0.044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 25.3min finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.5min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.549 (+/-0.043)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 50.2min remaining: 50.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.556 (+/-0.038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 50.5min finished\n"
     ]
    }
   ],
   "source": [
    "## depth\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 10, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 10, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   24.6s remaining:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   24.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.560 (+/-0.028)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 11.7min remaining: 11.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.548 (+/-0.033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 11.8min finished\n"
     ]
    }
   ],
   "source": [
    "## depth\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer(preprocessor=preprocessor)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(preprocessor=preprocessor,ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.4s remaining:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.558 (+/-0.034)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 91.0min remaining: 91.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.550 (+/-0.047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 92.1min finished\n"
     ]
    }
   ],
   "source": [
    "## depth\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,10))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('clf', XGBClassifier())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "                  ('clf', XGBClassifier())])\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "pipe2.fit(df_small['Page content'],df_small['Popularity'])\n",
    "\n",
    "#pred = pipe2.predict(testdata['Page content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23694882, 0.43114597, 0.6280522 , 0.29912624, 0.43623105,\n",
       "       0.38536134, 0.7120188 , 0.76651555, 0.7586255 , 0.5573365 ,\n",
       "       0.51771253, 0.37978962, 0.6109432 , 0.7578659 , 0.7346292 ,\n",
       "       0.48538724, 0.38706902, 0.72815496, 0.30742812, 0.4450541 ,\n",
       "       0.72462904, 0.36421612, 0.6487742 , 0.702284  , 0.30932677,\n",
       "       0.6284739 , 0.46544173, 0.23199305, 0.2529717 , 0.2797301 ,\n",
       "       0.6262077 , 0.7459523 , 0.3049826 , 0.67328054, 0.7114652 ,\n",
       "       0.21217795, 0.3653934 , 0.33806783, 0.43076172, 0.39572752,\n",
       "       0.39802375, 0.67044246, 0.8128762 , 0.29590946, 0.34572393,\n",
       "       0.47156206, 0.51480734, 0.34743178, 0.6768711 , 0.30221355,\n",
       "       0.23471804, 0.68349653, 0.5715238 , 0.6891289 , 0.6350345 ,\n",
       "       0.7238575 , 0.31274128, 0.6308068 , 0.42847425, 0.74643314,\n",
       "       0.7475513 , 0.36119676, 0.1928986 , 0.81937766, 0.26104608,\n",
       "       0.69364357, 0.42908227, 0.18135762, 0.46714747, 0.3988037 ,\n",
       "       0.51061916, 0.39538565, 0.41403446, 0.31881848, 0.34373873,\n",
       "       0.3388381 , 0.7134052 , 0.8725704 , 0.7815086 , 0.5838071 ,\n",
       "       0.6018372 , 0.6597211 , 0.33857945, 0.61004966, 0.6010205 ,\n",
       "       0.21488942, 0.7524475 , 0.32445502, 0.25198758, 0.6433322 ,\n",
       "       0.3502231 , 0.23559868, 0.72301525, 0.6273569 , 0.6026022 ,\n",
       "       0.5682136 , 0.7748284 , 0.74707615, 0.34589604, 0.8266596 ,\n",
       "       0.60243076, 0.24930948, 0.57363915, 0.30414873, 0.6437283 ,\n",
       "       0.47389624, 0.4298167 , 0.7099972 , 0.76416206, 0.22649333,\n",
       "       0.26423445, 0.26714572, 0.36394674, 0.38163376, 0.30750087,\n",
       "       0.82880646, 0.40361887, 0.38332525, 0.4351573 , 0.6913079 ,\n",
       "       0.6893674 , 0.57606834, 0.5369991 , 0.3093389 , 0.40035295,\n",
       "       0.7470069 , 0.36522433, 0.3641478 , 0.33407897, 0.6689343 ,\n",
       "       0.2815851 , 0.19402547, 0.24596845, 0.3520217 , 0.2194916 ,\n",
       "       0.29641575, 0.7767959 , 0.42381647, 0.38668695, 0.7268966 ,\n",
       "       0.6165722 , 0.8261786 , 0.71215314, 0.55909926, 0.7042872 ,\n",
       "       0.571556  , 0.8480045 , 0.27493802, 0.3140502 , 0.79720676,\n",
       "       0.32698494, 0.7714681 , 0.7110555 , 0.73073345, 0.15493421,\n",
       "       0.17023584, 0.6440326 , 0.6026842 , 0.32302895, 0.3111171 ,\n",
       "       0.42248607, 0.34172428, 0.85762554, 0.79267985, 0.35234597,\n",
       "       0.25529265, 0.704195  , 0.3134506 , 0.7464587 , 0.25543013,\n",
       "       0.43521485, 0.29668579, 0.4075847 , 0.7074584 , 0.59562564,\n",
       "       0.36977297, 0.28108066, 0.13188769, 0.70731336, 0.62428284,\n",
       "       0.61510855, 0.606535  , 0.6150123 , 0.7531615 , 0.57860285,\n",
       "       0.22647317, 0.14875963, 0.9374947 , 0.34873554, 0.44864666,\n",
       "       0.2749229 , 0.373763  , 0.42523175, 0.49114785, 0.40449306,\n",
       "       0.4408612 , 0.17873979, 0.68503606, 0.6243239 , 0.5517819 ,\n",
       "       0.24383214, 0.46388125, 0.6654416 , 0.20395336, 0.61541086,\n",
       "       0.86476046, 0.29227325, 0.8538859 , 0.67236006, 0.1784185 ,\n",
       "       0.29913855, 0.29554725, 0.82080287, 0.309357  , 0.35279754,\n",
       "       0.22079623, 0.43733424, 0.6771835 , 0.4230141 , 0.3527427 ,\n",
       "       0.5435127 , 0.2773871 , 0.7063469 , 0.8443796 , 0.74272496,\n",
       "       0.7938166 , 0.20277448, 0.52455395, 0.45919523, 0.08973376,\n",
       "       0.5836286 , 0.20664862, 0.8395768 , 0.4668011 , 0.65230966,\n",
       "       0.6460755 , 0.7200611 , 0.44194192, 0.2733377 , 0.44232702,\n",
       "       0.5749804 , 0.7800769 , 0.50127697, 0.64844173, 0.50887555,\n",
       "       0.6695202 , 0.76874495, 0.55788445, 0.41485643, 0.3216258 ,\n",
       "       0.17752562, 0.7240985 , 0.24179278, 0.613156  , 0.31006205,\n",
       "       0.3159265 , 0.43081814, 0.35801172, 0.3684927 , 0.71739423,\n",
       "       0.35612673, 0.18303667, 0.8740211 , 0.72428244, 0.3723399 ,\n",
       "       0.31505597, 0.41998774, 0.88364834, 0.19649673, 0.67638594,\n",
       "       0.6483834 , 0.6785387 , 0.36758706, 0.5922947 , 0.6665776 ,\n",
       "       0.33680823, 0.75674105, 0.26524624, 0.38521683, 0.65104383,\n",
       "       0.6578843 , 0.3430532 , 0.6699436 , 0.44009072, 0.73872787,\n",
       "       0.7148307 , 0.26472047, 0.869945  , 0.66745645, 0.51241136,\n",
       "       0.3148513 , 0.275118  , 0.252553  , 0.2911361 , 0.72923946,\n",
       "       0.59793127, 0.45167127, 0.727399  , 0.33398464, 0.26532623,\n",
       "       0.32654038, 0.663142  , 0.6709211 , 0.7515086 , 0.7602791 ,\n",
       "       0.34291887, 0.50548244, 0.35113758, 0.6912159 , 0.3959071 ,\n",
       "       0.6612651 , 0.22725216, 0.26778752, 0.47853747, 0.47683164,\n",
       "       0.34511426, 0.5699263 , 0.3499497 , 0.26424384, 0.20492937,\n",
       "       0.63188463, 0.27314055, 0.3805263 , 0.6378396 , 0.693392  ,\n",
       "       0.43954244, 0.27066246, 0.6256713 , 0.62948793, 0.4970631 ,\n",
       "       0.683132  , 0.39396232, 0.66238356, 0.50607   , 0.23452142,\n",
       "       0.407858  , 0.625765  , 0.26609334, 0.40120423, 0.2952021 ,\n",
       "       0.6458124 , 0.73517644, 0.37694576, 0.690912  , 0.42846045,\n",
       "       0.18296264, 0.7183868 , 0.6153057 , 0.39937598, 0.79334503,\n",
       "       0.95279974, 0.35183504, 0.73770154, 0.3680105 , 0.7396592 ,\n",
       "       0.15346366, 0.8603479 , 0.23643927, 0.59792805, 0.7206826 ,\n",
       "       0.2573736 , 0.36087868, 0.7328764 , 0.24855253, 0.652913  ,\n",
       "       0.60027593, 0.11457971, 0.2827964 , 0.2417327 , 0.739874  ,\n",
       "       0.8066101 , 0.6769197 , 0.57748574, 0.26331186, 0.6002519 ,\n",
       "       0.31068468, 0.33691636, 0.8770055 , 0.6641281 , 0.5145175 ,\n",
       "       0.5956503 , 0.7372465 , 0.6073274 , 0.5876461 , 0.2908021 ,\n",
       "       0.7652819 , 0.19246352, 0.67941177, 0.38591516, 0.23072693,\n",
       "       0.37160164, 0.42059487, 0.8129541 , 0.6402163 , 0.69441   ,\n",
       "       0.28399545, 0.18838826, 0.19083083, 0.39519355, 0.22428699,\n",
       "       0.6803468 , 0.31342313, 0.79095995, 0.29759958, 0.65279764,\n",
       "       0.7620001 , 0.8575492 , 0.27918023, 0.32423368, 0.5057424 ,\n",
       "       0.51062965, 0.76741993, 0.40318274, 0.7991659 , 0.28200546,\n",
       "       0.873958  , 0.763067  , 0.66131943, 0.66205245, 0.56789386,\n",
       "       0.36095348, 0.5772959 , 0.40909222, 0.22303978, 0.7807238 ,\n",
       "       0.49092132, 0.5805885 , 0.25290135, 0.7882286 , 0.79818016,\n",
       "       0.53012365, 0.38735473, 0.68598515, 0.18989278, 0.79179734,\n",
       "       0.8260078 , 0.34012142, 0.7357313 , 0.76201624, 0.5585868 ,\n",
       "       0.34137046, 0.19379394, 0.76935977, 0.28905088, 0.7652251 ,\n",
       "       0.61119694, 0.41556486, 0.7501469 , 0.6081558 , 0.53411144,\n",
       "       0.15114948, 0.74183184, 0.67325675, 0.7354311 , 0.60174376,\n",
       "       0.746984  , 0.5447357 , 0.3744316 , 0.24967118, 0.7495941 ,\n",
       "       0.28792033, 0.52608   , 0.12600248, 0.28941992, 0.72670436,\n",
       "       0.24117869, 0.82265985, 0.69960535, 0.5497216 , 0.57061565,\n",
       "       0.68927824, 0.2574351 , 0.73195225, 0.61263055, 0.46261588,\n",
       "       0.28168184, 0.8131685 , 0.8126494 , 0.25672632, 0.44149444,\n",
       "       0.66558707, 0.54060197, 0.32798874, 0.86749965, 0.6162243 ,\n",
       "       0.20483768, 0.77773154, 0.62431645, 0.7493788 , 0.55682886,\n",
       "       0.3175995 , 0.73211193, 0.85860175, 0.5063052 , 0.8616868 ,\n",
       "       0.32959843, 0.38802135, 0.69104123, 0.71268386, 0.6252874 ,\n",
       "       0.7817162 , 0.5920279 , 0.36489013, 0.28813416, 0.2433941 ,\n",
       "       0.28651357, 0.28845912, 0.30141622, 0.83034396, 0.4851235 ,\n",
       "       0.3751964 , 0.7277624 , 0.29157993, 0.8039205 , 0.6695967 ,\n",
       "       0.6283007 , 0.6963744 , 0.7192772 , 0.8251538 , 0.3103907 ,\n",
       "       0.7072916 , 0.3665005 , 0.1965879 , 0.6454986 , 0.7806671 ,\n",
       "       0.38609943, 0.41640687, 0.6817798 , 0.85960734, 0.7458575 ,\n",
       "       0.17017972, 0.37209818, 0.3471358 , 0.5797965 , 0.5233916 ,\n",
       "       0.35303068, 0.42766744, 0.3615199 , 0.6696278 , 0.43996018,\n",
       "       0.65819746, 0.69518584, 0.52162826, 0.13856557, 0.3893424 ,\n",
       "       0.2896346 , 0.83914953, 0.6424876 , 0.7147249 , 0.26513934,\n",
       "       0.41132364, 0.80426353, 0.45607045, 0.51895905, 0.7031866 ,\n",
       "       0.48172656, 0.48312703, 0.6505039 , 0.236416  , 0.21824941,\n",
       "       0.5705763 , 0.638023  , 0.89874107, 0.29321116, 0.31796634,\n",
       "       0.23231149, 0.23338954, 0.90602857, 0.7409358 , 0.11713255,\n",
       "       0.4628982 , 0.66981256, 0.39873487, 0.27422878, 0.59225255,\n",
       "       0.6752434 , 0.35560665, 0.22848028, 0.21461298, 0.83661824,\n",
       "       0.4485786 , 0.40410772, 0.681369  , 0.27705875, 0.6023513 ,\n",
       "       0.40885594, 0.43181592, 0.78463066, 0.7082917 , 0.93373156,\n",
       "       0.818893  , 0.79546005, 0.33276737, 0.56844425, 0.3247895 ,\n",
       "       0.5949169 , 0.81600446, 0.5678704 , 0.79057133, 0.38382843,\n",
       "       0.39169663, 0.62669504, 0.88226   , 0.34541336, 0.31484655,\n",
       "       0.35179994, 0.5724328 , 0.19222905, 0.12632301, 0.5946345 ,\n",
       "       0.36667535, 0.37265438, 0.6483329 , 0.25476754, 0.6300468 ,\n",
       "       0.3601391 , 0.610593  , 0.6615733 , 0.26046526, 0.40419024,\n",
       "       0.3111336 , 0.49691296, 0.6267717 , 0.8355248 , 0.74763143,\n",
       "       0.6426014 , 0.49621055, 0.27202785, 0.349081  , 0.22179252,\n",
       "       0.36567026, 0.789427  , 0.4963524 , 0.46654505, 0.6338203 ,\n",
       "       0.6986956 , 0.25378197, 0.78805935, 0.73652184, 0.27542856,\n",
       "       0.75577503, 0.6849483 , 0.13128652, 0.85509217, 0.632394  ,\n",
       "       0.49565798, 0.384492  , 0.81071895, 0.4080784 , 0.3354494 ,\n",
       "       0.71731645, 0.7211482 , 0.16755717, 0.805161  , 0.8601457 ,\n",
       "       0.48452535, 0.37120852, 0.32135814, 0.7882193 , 0.6837924 ,\n",
       "       0.3027609 , 0.7091211 , 0.6376266 , 0.51302135, 0.5662062 ,\n",
       "       0.836508  , 0.31579283, 0.8044959 , 0.42156276, 0.77649444,\n",
       "       0.774453  , 0.4794764 , 0.55744   , 0.34226605, 0.86201036,\n",
       "       0.20664874, 0.735586  , 0.29903644, 0.77987087, 0.22664337,\n",
       "       0.30752906, 0.8201328 , 0.36780924, 0.6903758 , 0.42125937,\n",
       "       0.23027389, 0.16192766, 0.2564622 , 0.5857851 , 0.5857093 ,\n",
       "       0.29898497, 0.8074442 , 0.23430386, 0.35583752, 0.4144055 ,\n",
       "       0.2579133 , 0.2407089 , 0.603186  , 0.18356442, 0.7324041 ,\n",
       "       0.754876  , 0.30122077, 0.35691544, 0.07827782, 0.7519717 ,\n",
       "       0.26340616, 0.40563014, 0.29118937, 0.5377153 , 0.73382664,\n",
       "       0.43141377, 0.7322176 , 0.7557432 , 0.28004286, 0.20460558,\n",
       "       0.21017738, 0.44948262, 0.37781444, 0.6990413 , 0.26703107,\n",
       "       0.6634107 , 0.2655002 , 0.66883177, 0.6032043 , 0.37221393,\n",
       "       0.36493564, 0.6719018 , 0.40678123, 0.7466627 , 0.30811104,\n",
       "       0.24394265, 0.35819232, 0.41611823, 0.23537937, 0.29088247,\n",
       "       0.5985981 , 0.68177813, 0.15889604, 0.33817086, 0.4950061 ,\n",
       "       0.6590745 , 0.8063971 , 0.14221841, 0.28744233, 0.53817886,\n",
       "       0.7764544 , 0.18317659, 0.715214  , 0.7147576 , 0.24702996,\n",
       "       0.37807706, 0.74069494, 0.7557276 , 0.221537  , 0.7886327 ,\n",
       "       0.43092808, 0.5157223 , 0.8213057 , 0.6635718 , 0.23566492,\n",
       "       0.42634106, 0.54101384, 0.38505557, 0.7880293 , 0.78200674,\n",
       "       0.7257758 , 0.57456607, 0.2918383 , 0.31987745, 0.7986493 ,\n",
       "       0.31756416, 0.5842192 , 0.38886017, 0.4830873 , 0.65860516,\n",
       "       0.50650895, 0.23497939, 0.31981474, 0.5633435 , 0.37552392,\n",
       "       0.7050455 , 0.68925273, 0.6211969 , 0.6111117 , 0.39032486,\n",
       "       0.5193406 , 0.8255567 , 0.31110072, 0.4305602 , 0.8621479 ,\n",
       "       0.68142337, 0.29569653, 0.62543803, 0.37075147, 0.5547743 ,\n",
       "       0.70149577, 0.5535076 , 0.8260105 , 0.67656523, 0.40912536,\n",
       "       0.47602037, 0.398854  , 0.6571029 , 0.3547021 , 0.6098989 ,\n",
       "       0.6894274 , 0.38561857, 0.41905987, 0.29128894, 0.61909354,\n",
       "       0.28947243, 0.48786867, 0.8141611 , 0.5938803 , 0.5657542 ,\n",
       "       0.6875208 , 0.84598035, 0.45276484, 0.76024747, 0.41565073,\n",
       "       0.38551715, 0.42298636, 0.8970788 , 0.43089142, 0.64397633,\n",
       "       0.85368806, 0.4579223 , 0.40281937, 0.61410165, 0.46588686,\n",
       "       0.77622175, 0.7831959 , 0.38846982, 0.32819235, 0.5227052 ,\n",
       "       0.6538487 , 0.7572352 , 0.3878321 , 0.2555444 , 0.6974956 ,\n",
       "       0.31114474, 0.82307196, 0.7420515 , 0.32074273, 0.30291307,\n",
       "       0.2382432 , 0.3922463 , 0.7191773 , 0.6023011 , 0.50299585,\n",
       "       0.3733563 , 0.33012137, 0.80320793, 0.9299882 , 0.58473533,\n",
       "       0.37525007, 0.30996424, 0.378961  , 0.73112434, 0.74492306,\n",
       "       0.4325408 , 0.40889138, 0.6299597 , 0.51511014, 0.37486988,\n",
       "       0.7041073 , 0.8726604 , 0.3978201 , 0.32016426, 0.25261796,\n",
       "       0.77409047, 0.51258004, 0.1872826 , 0.4446264 , 0.43865195,\n",
       "       0.3672585 , 0.41867435, 0.74489576, 0.25252423, 0.3683899 ,\n",
       "       0.34900084, 0.39055467, 0.40692797, 0.25776833, 0.6521107 ,\n",
       "       0.5852927 , 0.3556065 , 0.22781229, 0.35477743, 0.29662675,\n",
       "       0.49379423, 0.2723144 , 0.66611964, 0.737413  , 0.62280077,\n",
       "       0.31121084, 0.39412734, 0.57545716, 0.33842534, 0.4574444 ,\n",
       "       0.5548357 , 0.16713758, 0.2381104 , 0.38203868, 0.705188  ,\n",
       "       0.7553536 , 0.6952008 , 0.735018  , 0.37359536, 0.84986097,\n",
       "       0.30902424, 0.5612644 , 0.7435409 , 0.22427155, 0.37796354,\n",
       "       0.3723595 , 0.6822234 , 0.40222803, 0.20186312, 0.3343423 ,\n",
       "       0.69396085, 0.4482856 , 0.3213096 , 0.32273167, 0.3756012 ,\n",
       "       0.63637275, 0.7124798 , 0.692958  , 0.41207984, 0.4275989 ,\n",
       "       0.37988183, 0.5163477 , 0.2567505 , 0.29981795, 0.294315  ,\n",
       "       0.31732962, 0.68389463, 0.68010914, 0.8639695 , 0.35388958,\n",
       "       0.24003454, 0.71154   , 0.39705208, 0.6525341 , 0.7780144 ,\n",
       "       0.7606906 , 0.3027095 , 0.3948652 , 0.6505393 , 0.63042265,\n",
       "       0.35695285, 0.6117107 , 0.8929331 , 0.21652322, 0.6243766 ,\n",
       "       0.20480262, 0.773354  , 0.68009543, 0.3319075 , 0.8433348 ,\n",
       "       0.87904483, 0.754862  , 0.35157403, 0.6217495 , 0.4505428 ,\n",
       "       0.5365157 , 0.19846262, 0.31953913, 0.6323659 , 0.30213505,\n",
       "       0.80162513, 0.47981954, 0.20850329, 0.31775448, 0.532445  ,\n",
       "       0.2442612 , 0.7485226 , 0.76348084, 0.7145328 , 0.3272387 ,\n",
       "       0.6109212 , 0.58168805, 0.31636605, 0.7915603 , 0.28044587,\n",
       "       0.5202693 , 0.7195454 , 0.7331673 , 0.286313  , 0.54419124,\n",
       "       0.80722547, 0.55985606, 0.7631805 , 0.43848452, 0.23403144,\n",
       "       0.53147984, 0.2565657 , 0.5607995 , 0.28058797, 0.70308214],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.predict_proba(df_small['Page content'])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    6.8s remaining:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.542 (+/-0.039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   22.6s remaining:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   23.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.530 (+/-0.037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.5s remaining:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.503 (+/-0.041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   17.1s remaining:   17.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.498 (+/-0.041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', RandomForestClassifier())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_dept=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_dept=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_dept=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    5.9s remaining:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.539 (+/-0.027)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   13.8s remaining:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   15.3s remaining:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   15.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   14.9s remaining:   14.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   15.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', RandomForestClassifier())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_depth=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_depth=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', RandomForestClassifier(n_estimators=150,max_depth=5,min_samples_split=1,min_samples_leaf=0.5,n_jobs=-1,verbose=3))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-f925e31ae2e4>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-f925e31ae2e4>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    ('clf', from xgboost import XGBClassifier\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.7))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))(colsample_bytree = 0.5, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.7))])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost pipe2 Kaggle 0.56985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3a8b80fd8562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m          'XGBClassifier+(1,5)gram']\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# without preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m pipe1 = Pipeline([('vect', CountVectorizer()), \n\u001b[0m\u001b[0;32m     12\u001b[0m                   ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# without preprocessing, use larger ngram range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2,pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   42.6s remaining:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   43.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.519 (+/-0.029)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   21.2s remaining:   21.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.540 (+/-0.062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   21.9s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "# without preprocessing\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19:46~23:02\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=1))])\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "pred = pipe2.predict_proba(testdata['Page content'])[:,1]\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v6.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   31.4s remaining:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   31.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.562 (+/-0.058)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 17.3min remaining: 17.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.570 (+/-0.031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 17.4min finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.0min remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.510 (+/-0.048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   22.7s remaining:   22.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,5)gram: 0.538 (+/-0.053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   22.9s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,5)gram']\n",
    "\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 10, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 10, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5))])\n",
    "# CV\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost pipe1 0.56441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   33.2s remaining:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   33.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.564 (+/-0.060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 17.3min remaining: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 17.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.570 (+/-0.030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   41.0s remaining:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   41.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.517 (+/-0.030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   22.2s remaining:   22.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.540 (+/-0.062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   22.9s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,5))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2,pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 247.28 seconds\n",
      "time elapsed: 13.64 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "pipe1.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pred = pipe1.predict_proba(testdata['Page content'])[:,1]\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v7.csv',index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost pip2 0.57007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   33.1s remaining:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   33.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.564 (+/-0.060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  7.1min remaining:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.569 (+/-0.033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   30.8s remaining:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   31.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.516 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   21.4s remaining:   21.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.511 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   21.9s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2,pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 4088.09 seconds\n",
      "time elapsed: 48.61 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pred = pipe2.predict_proba(testdata['Page content'])[:,1]\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v8.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['Popularity']) \n",
    "y[y==-1]=0\n",
    "df['Popularity'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "27638    0\n",
       "27639    0\n",
       "27640    0\n",
       "27641    0\n",
       "27642    1\n",
       "Name: Popularity, Length: 27643, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   35.8s remaining:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   36.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.564 (+/-0.060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  7.2min remaining:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.569 (+/-0.033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   30.8s remaining:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   31.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.516 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   21.4s remaining:   21.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.511 (+/-0.040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   22.4s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "df_small['Popularity']\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 5, min_child_weight = 1e-05, n_estimators =100, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2,pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13516    0\n",
       "11607    0\n",
       "11866    1\n",
       "25540    0\n",
       "26611    0\n",
       "        ..\n",
       "12673    0\n",
       "14569    0\n",
       "22266    1\n",
       "19962    0\n",
       "17686    1\n",
       "Name: Popularity, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  2.1min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.551 (+/-0.046)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 31.6min remaining: 31.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 31.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+(1,2)gram: 0.567 (+/-0.042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess: 0.514 (+/-0.047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   31.0s remaining:   31.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier+preprocess+hash: 0.506 (+/-0.035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   31.6s finished\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "df_small['Popularity']\n",
    "names = ['XGBClassifier', \n",
    "         'XGBClassifier+(1,2)gram',\n",
    "         'XGBClassifier+preprocess',\n",
    "         'XGBClassifier+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 15, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 15, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 15, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 15, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "# CV\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2,pipe3,pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "df_small = df.sample(n=3000, random_state=0)\n",
    "scores = cross_val_score(estimator=pipe2, X=df_small['Page content'], y=df_small['Popularity'], \\\n",
    "                         cv=10, scoring='roc_auc',n_jobs=-1,verbose=2)\n",
    "print('%s: %.3f (+/-%.3f)' % ('XGBClassifier+(1,3)gram', scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 21354.36 seconds\n",
      "time elapsed: 45.39 seconds\n"
     ]
    }
   ],
   "source": [
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 15, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pred = pipe2.predict_proba(testdata['Page content'])[:,1]\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v9.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 29292.52 seconds\n",
      "time elapsed: 49.86 seconds\n"
     ]
    }
   ],
   "source": [
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', XGBClassifier(colsample_bytree = 1, learning_rate = 0.001, max_depth = 20, min_child_weight = 1e-05, n_estimators =150, random_state= 0,subsample=0.5,max_delta_step=1,gamma=0.5))])\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "pipe2.fit(df['Page content'],df['Popularity'])\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pred = pipe2.predict_proba(testdata['Page content'])[:,1]\n",
    "\n",
    "t2 = time.time()\n",
    "print('time elapsed: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "\n",
    "\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testdata['Id']\n",
    "pred_csv['Popularity'] = pred\n",
    "pd.DataFrame(pred_csv).to_csv('C:/Users/stat_pc/Desktop/深度學習/Competition 01/y_pred_v10.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.7.9v1]",
   "language": "python",
   "name": "conda-env-python3.7.9v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
