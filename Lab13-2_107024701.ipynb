{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 10 23:33:39 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0 Off |                  N/A |\r\n",
      "| 43%   71C    P8     4W / 250W |    835MiB / 11016MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil  \n",
    "# shutil.rmtree('Lab13-2 dataset')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile as zf\n",
    "# files = zf.ZipFile(\"words_captcha.zip\", 'r')\n",
    "# files.extractall('Lab13-2 dataset')\n",
    "# files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、載入data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=[]\n",
    "final_label =[]\n",
    "path = 'Lab13-2 dataset/words_captcha/spec_train_val.txt'\n",
    "f = open(path, 'r')\n",
    "for label in f:\n",
    "    id.append(label.split()[0])\n",
    "    new_label = label.split()[1][0]\n",
    "    for j in range(1,len(label.split()[1])):\n",
    "        \n",
    "        temp = label.split()[1][j]\n",
    "        new_label = new_label+\" \"+temp\n",
    "    caption = '<start> ' + new_label + ' <end>'\n",
    "    final_label.append(caption)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "for i in range(140000):\n",
    "    image.append(\"Lab13-2 dataset/words_captcha/\"+\"a\"+str(i)+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = final_label\n",
    "img_name_vector = image\n",
    "\n",
    "num_examples = 120000\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 120000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions), len(img_name_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path,cap):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (160, 300))\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    img = img/255*2-1\n",
    "    #img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets using an 80-20 split\n",
    "img_name_train =img_name_vector[:100000]\n",
    "img_name_val = img_name_vector[100000:]                                                                \n",
    "cap_train = cap_vector[:100000]\n",
    "cap_val = cap_vector[100000:]                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000, 20000, 20000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "num_steps = len(img_name_val) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "#dataset = dataset.map(load_image)\n",
    "dataset = dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "#val_dataset = val_dataset.map(load_image)\n",
    "val_dataset = val_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Shuffle and batch\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder  內有自己設計的架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim,**kwargs):\n",
    "        super(CNN_Encoder, self).__init__(**kwargs)\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.c1 = tf.keras.layers.Conv2D(32, 3, strides=1,activation=\"relu\")\n",
    "        self.m1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.c2 = tf.keras.layers.Conv2D(64, 3, strides=1,activation=\"relu\")\n",
    "        self.m2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.c3 = tf.keras.layers.Conv2D(128, 3, strides=1,activation=\"relu\")\n",
    "        self.m3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.c4 = tf.keras.layers.Conv2D(256, 3, strides=1,activation=\"relu\")\n",
    "        self.m4 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.c5 = tf.keras.layers.Conv2D(embedding_dim, 3, strides=1,activation=\"relu\")\n",
    "        self.m5 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.m1(x)\n",
    "        \n",
    "        x = self.c2(x)\n",
    "        x = self.m2(x)\n",
    "        \n",
    "        x = self.c3(x)\n",
    "        x = self.m3(x)\n",
    "        \n",
    "        \n",
    "        x = self.c4(x)\n",
    "        x = self.m4(x)\n",
    "        \n",
    "        x = self.c5(x)\n",
    "        x = self.m5(x)\n",
    "        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train17\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、開始training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(img_tensor, target):\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "   \n",
    "    pred_result = tf.zeros((target.shape[0], 1),dtype=tf.float32)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        dec_input = tf.expand_dims(tf.cast(tf.argmax(predictions, axis=1),tf.float32), 1)\n",
    "       \n",
    "        pred_result = tf.concat((pred_result,dec_input),axis=1)\n",
    "   \n",
    "    Target = target[:,1:]\n",
    "    Pred = pred_result[:,1:]\n",
    "   \n",
    "    mask = tf.math.logical_not(tf.math.equal(Target, 0))\n",
    "   \n",
    "    mask = tf.cast(mask, dtype = Pred.dtype)\n",
    "    Target = tf.cast(Target, dtype = Pred.dtype)\n",
    "   \n",
    "    Pred *= mask\n",
    "   \n",
    "    is_the_same = tf.reduce_all(tf.math.equal(Pred, Target), axis=1)\n",
    "    acc = tf.math.reduce_mean(tf.cast(is_the_same, tf.float32))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation accuracy 超過0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 8.429667\n",
      "accuracy 0.014599990099668503\n",
      "Time taken for 1 epoch 95.50644779205322 sec\n",
      "\n",
      "Epoch 2 Loss 3.223623\n",
      "accuracy 0.48820003867149353\n",
      "Time taken for 1 epoch 86.61629700660706 sec\n",
      "\n",
      "Epoch 3 Loss 1.083975\n",
      "accuracy 0.7585500478744507\n",
      "Time taken for 1 epoch 85.8757905960083 sec\n",
      "\n",
      "Epoch 4 Loss 0.568286\n",
      "accuracy 0.8351500034332275\n",
      "Time taken for 1 epoch 85.24988770484924 sec\n",
      "\n",
      "Epoch 5 Loss 0.359620\n",
      "accuracy 0.8689499497413635\n",
      "Time taken for 1 epoch 85.27937841415405 sec\n",
      "\n",
      "Epoch 6 Loss 0.272444\n",
      "accuracy 0.8735998272895813\n",
      "Time taken for 1 epoch 85.13059377670288 sec\n",
      "\n",
      "Epoch 7 Loss 0.267327\n",
      "accuracy 0.8988998532295227\n",
      "Time taken for 1 epoch 84.03714919090271 sec\n",
      "\n",
      "Epoch 8 Loss 0.184461\n",
      "accuracy 0.9105001091957092\n",
      "Time taken for 1 epoch 85.02520203590393 sec\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-76952bcf53c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maccur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "    accur = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "         accur += validation_step(img_tensor, target)\n",
    "#         if batch % 100 == 0:\n",
    "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "    \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('accuracy {}'.format(accur/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data 輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_vector = []\n",
    "cap_test = []\n",
    "for i in range(120000,140000):\n",
    "    test_name_vector.append(\"Lab13-2 dataset/words_captcha/\"+'a'+str(i)+'.png')\n",
    "    cap_test.append('<start>')\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((test_name_vector, cap_test))\n",
    "dataset_test = dataset_test.map(load_image).batch(BATCH_SIZE)\n",
    "\n",
    "output_file = open('./test_prediction.txt', 'w')\n",
    "for (batch_test, (img_tensor_test, target_test)) in enumerate(dataset_test):\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor_test)\n",
    "\n",
    "    pred_result = tf.zeros((target_test.shape[0], 1),dtype=tf.float32)\n",
    "    for i in range(1, 7):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        dec_input = tf.expand_dims(tf.cast(tf.argmax(predictions, axis=1),tf.float32), 1)\n",
    "\n",
    "        pred_result = tf.concat((pred_result,dec_input),axis=1)\n",
    "    pred = pred_result[:,1:]\n",
    "    mask = tf.math.logical_not(tf.math.equal(pred, 0))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        output = 'a'+str(batch_test*BATCH_SIZE+i+120000)+'.png'\n",
    "        pred_index = tf.cast(pred[i],dtype = tf.int32).numpy()\n",
    "        pred_index_clear = []\n",
    "        for j in (pred_index):\n",
    "            if(j == 2):\n",
    "                break\n",
    "            pred_index_clear.append(j)\n",
    "        pred_str = [tokenizer.index_word[j] for j in pred_index_clear]\n",
    "        output =output+' '+(''.join(pred_str))\n",
    "        output_file.write(output+'\\n')\n",
    "output_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
